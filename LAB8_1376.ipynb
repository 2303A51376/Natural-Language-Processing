{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODraSREsVXoAF2+kjSVUIe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51376/Natural-Language-Processing/blob/main/LAB8_1376.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGGT2a_EgTfM",
        "outputId": "f060b379-9478-4fb6-d339-8604b8240717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 18846 documents.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "# If not already downloaded, download NLTK data (only needed once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # WordNet multilingual data (helps sometimes)\n",
        "\n",
        "# Load 20 newsgroups dataset (remove headers/footers/quotes to focus on content)\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'))\n",
        "documents = newsgroups.data  # list of raw strings\n",
        "print(f\"Loaded {len(documents)} documents.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Option A: spaCy lemmatization (preferred if spaCy installed)\n",
        "USE_SPACY = False  # set True if you installed spaCy and the model\n",
        "\n",
        "if USE_SPACY:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"parser\"])\n",
        "    def preprocess_doc_spacy(doc, extra_stop=set()):\n",
        "        doc = doc.lower()\n",
        "        # remove urls and emails and numeric tokens\n",
        "        doc = re.sub(r'(http\\S+)|(\\S+@\\S+)|\\d+', ' ', doc)\n",
        "        doc = re.sub(r'[^a-z\\s]', ' ', doc)\n",
        "        sp = nlp(doc)\n",
        "        lemmas = [token.lemma_ for token in sp if len(token.lemma_)>1 and not token.is_stop]\n",
        "        return \" \".join([w for w in lemmas if w not in extra_stop])\n",
        "\n",
        "# Option B: NLTK WordNet lemmatizer (always works)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def simple_preprocess(doc, extra_stop=set()):\n",
        "    # Lowercase\n",
        "    doc = doc.lower()\n",
        "    # remove urls/emails/numbers\n",
        "    doc = re.sub(r'(http\\S+)|(\\S+@\\S+)|\\d+', ' ', doc)\n",
        "    # keep only letters\n",
        "    doc = re.sub(r'[^a-z\\s]', ' ', doc)\n",
        "    tokens = doc.split()\n",
        "    # remove stopwords and short tokens and lemmatize\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    lemmas = [w for w in lemmas if w not in extra_stop]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# Apply preprocessing to all documents (this may take some time)\n",
        "clean_docs = [simple_preprocess(doc) for doc in documents]\n",
        "print(\"Preprocessing done. Example cleaned doc:\")\n",
        "print(clean_docs[0][:400])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBhmCxUgggIb",
        "outputId": "3f3a5358-a92f-4398-ffd7-348440710de2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing done. Example cleaned doc:\n",
            "sure bashers pen fan pretty confused lack kind post recent pen massacre devil actually bit puzzled bit relieved however going put end non pittsburghers relief bit praise pen man killing devil worse thought jagr showed much better regular season stats also lot fun watch playoff bowman let jagr lot fun next couple game since pen going beat pulp jersey anyway disappointed see islander lose final regu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_features = 10000  # vocabulary size; adjust downward if memory limited\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, max_features=n_features)\n",
        "tfidf = tfidf_vectorizer.fit_transform(clean_docs)  # shape (n_docs, n_features)\n",
        "\n",
        "count_vectorizer = CountVectorizer(max_df=0.95, min_df=5, max_features=n_features)\n",
        "counts = count_vectorizer.fit_transform(clean_docs)\n",
        "\n",
        "print(\"TF-IDF shape:\", tfidf.shape)\n",
        "print(\"Count shape:\", counts.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSLAFj8QgxcP",
        "outputId": "b77ac073-cb10-4063-91ea-6a532c35586e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shape: (18846, 10000)\n",
            "Count shape: (18846, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "n_topics = 5\n",
        "n_top_words = 10\n",
        "\n",
        "# NMF on TF-IDF\n",
        "nmf = NMF(n_components=n_topics, random_state=42, init='nndsvda', max_iter=400)\n",
        "W = nmf.fit_transform(tfidf)  # document-topic matrix\n",
        "H = nmf.components_          # topic-term matrix\n",
        "\n",
        "# LDA on counts\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch', max_iter=10)\n",
        "lda_doc_topic = lda.fit_transform(counts)\n",
        "lda_components = lda.components_\n",
        "\n",
        "# Utility to get top words\n",
        "def show_top_words(model_components, feature_names, n_top_words):\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(model_components):\n",
        "        top_indices = topic.argsort()[::-1][:n_top_words]\n",
        "        top_words = [feature_names[i] for i in top_indices]\n",
        "        topics.append(top_words)\n",
        "    return topics\n",
        "\n",
        "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
        "count_features = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "nmf_topics = show_top_words(H, tfidf_features, n_top_words)\n",
        "lda_topics = show_top_words(lda_components, count_features, n_top_words)\n",
        "\n",
        "print(\"=== NMF topics (top words) ===\")\n",
        "for i, words in enumerate(nmf_topics):\n",
        "    print(f\"Topic {i+1}:\", \", \".join(words))\n",
        "\n",
        "print(\"\\n=== LDA topics (top words) ===\")\n",
        "for i, words in enumerate(lda_topics):\n",
        "    print(f\"Topic {i+1}:\", \", \".join(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjJUgFQhGZI",
        "outputId": "8180c055-ea6e-47b4-e255-f17db6865b69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NMF topics (top words) ===\n",
            "Topic 1: would, one, people, like, think, get, right, time, know, thing\n",
            "Topic 2: window, file, thanks, please, program, anyone, know, mail, do, driver\n",
            "Topic 3: game, team, year, player, hockey, season, play, baseball, win, last\n",
            "Topic 4: drive, scsi, disk, card, hard, controller, ide, floppy, mac, meg\n",
            "Topic 5: god, christian, jesus, bible, believe, christ, say, belief, faith, sin\n",
            "\n",
            "=== LDA topics (top words) ===\n",
            "Topic 1: window, one, file, use, drive, get, would, problem, like, know\n",
            "Topic 2: one, would, people, god, like, think, know, say, time, thing\n",
            "Topic 3: game, one, would, year, get, team, time, gun, people, like\n",
            "Topic 4: system, key, file, space, information, available, use, program, data, also\n",
            "Topic 5: max, armenian, state, year, people, would, government, right, president, one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def topic_overlap(top_words_a, top_words_b):\n",
        "    set_a = set(top_words_a)\n",
        "    set_b = set(top_words_b)\n",
        "    return len(set_a & set_b) / len(set_a | set_b)  # Jaccard-like overlap\n",
        "\n",
        "overlap_matrix = np.zeros((n_topics, n_topics))\n",
        "for i in range(n_topics):\n",
        "    for j in range(n_topics):\n",
        "        overlap_matrix[i,j] = topic_overlap(nmf_topics[i], lda_topics[j])\n",
        "\n",
        "print(\"Overlap matrix (rows=NMF topics, cols=LDA topics):\")\n",
        "print(np.round(overlap_matrix, 2))\n",
        "\n",
        "# For each NMF topic, show best matching LDA topic\n",
        "for i in range(n_topics):\n",
        "    best_j = overlap_matrix[i].argmax()\n",
        "    print(f\"NMF Topic {i+1} best matches LDA Topic {best_j+1} with overlap {overlap_matrix[i,best_j]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Ym8sPDhha3",
        "outputId": "d41200ab-2c06-4172-aa0d-b729e88571b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap matrix (rows=NMF topics, cols=LDA topics):\n",
            "[[0.33 0.67 0.43 0.   0.25]\n",
            " [0.18 0.05 0.   0.11 0.  ]\n",
            " [0.   0.   0.18 0.   0.05]\n",
            " [0.05 0.   0.   0.   0.  ]\n",
            " [0.   0.11 0.   0.   0.  ]]\n",
            "NMF Topic 1 best matches LDA Topic 2 with overlap 0.67\n",
            "NMF Topic 2 best matches LDA Topic 1 with overlap 0.18\n",
            "NMF Topic 3 best matches LDA Topic 3 with overlap 0.18\n",
            "NMF Topic 4 best matches LDA Topic 1 with overlap 0.05\n",
            "NMF Topic 5 best matches LDA Topic 2 with overlap 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def max_similarity_between_words(word1, word2, metric='wup'):\n",
        "    # get all synsets for each word (noun synsets preferred but we'll check all)\n",
        "    synsets1 = wn.synsets(word1)\n",
        "    synsets2 = wn.synsets(word2)\n",
        "    if not synsets1 or not synsets2:\n",
        "        return None, None, None  # not found\n",
        "\n",
        "    best_sim = -1.0\n",
        "    best_pair = (None, None)\n",
        "    best_path = None\n",
        "    for s1 in synsets1:\n",
        "        for s2 in synsets2:\n",
        "            if metric == 'wup':\n",
        "                sim = s1.wup_similarity(s2)\n",
        "            elif metric == 'path':\n",
        "                sim = s1.path_similarity(s2)\n",
        "            else:\n",
        "                raise ValueError(\"metric must be 'wup' or 'path'\")\n",
        "            if sim is None:\n",
        "                continue\n",
        "            if sim > best_sim:\n",
        "                best_sim = sim\n",
        "                best_pair = (s1, s2)\n",
        "    return best_sim, best_pair[0], best_pair[1]\n",
        "\n",
        "# Example: choose two words from a chosen topic (modify indices as you like)\n",
        "# We'll pick the first NMF topic's top two words for demonstration:\n",
        "word_a = nmf_topics[0][0]\n",
        "word_b = nmf_topics[0][1]\n",
        "print(\"Comparing:\", word_a, \"AND\", word_b)\n",
        "\n",
        "wup_sim, s1, s2 = max_similarity_between_words(word_a, word_b, metric='wup')\n",
        "path_sim, _, _ = max_similarity_between_words(word_a, word_b, metric='path')\n",
        "\n",
        "print(\"Wu-Palmer similarity:\", wup_sim)\n",
        "print(\"Path similarity:\", path_sim)\n",
        "print(\"Best synset pair:\", s1, s2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5GUJ54BhlNx",
        "outputId": "8a723d29-3f13-43de-9fea-f9870ca1eaaa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing: would AND one\n",
            "Wu-Palmer similarity: None\n",
            "Path similarity: None\n",
            "Best synset pair: None None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def jaccard(a, b):\n",
        "    set_a = set(a.split())\n",
        "    set_b = set(b.split())\n",
        "    if not set_a and not set_b:\n",
        "        return 1.0\n",
        "    if not set_a or not set_b:\n",
        "        return 0.0\n",
        "    return len(set_a & set_b) / len(set_a | set_b)\n",
        "\n",
        "# Choose three document indices (you can change these to inspect different docs)\n",
        "doc_indices = [10, 25, 200]  # example indices; change as you like\n",
        "\n",
        "docs_selected = [clean_docs[i] for i in doc_indices]\n",
        "for idx, text in zip(doc_indices, docs_selected):\n",
        "    # show top keywords by term frequency in that doc\n",
        "    tokens = text.split()\n",
        "    most_common = Counter(tokens).most_common(10)\n",
        "    print(f\"\\nDocument index: {idx}\")\n",
        "    print(\"Top tokens:\", [t for t,c in most_common])\n",
        "\n",
        "# compute pairwise Jaccard\n",
        "pairs = list(combinations(range(3), 2))\n",
        "scores = {}\n",
        "for (i,j) in pairs:\n",
        "    score = jaccard(docs_selected[i], docs_selected[j])\n",
        "    scores[(doc_indices[i], doc_indices[j])] = score\n",
        "    print(f\"Jaccard similarity between doc {doc_indices[i]} and doc {doc_indices[j]}: {score:.3f}\")\n",
        "\n",
        "# identify most and least similar\n",
        "most_sim_pair = max(scores.items(), key=lambda x:x[1])\n",
        "least_sim_pair = min(scores.items(), key=lambda x:x[1])\n",
        "\n",
        "print(\"\\nMost similar pair:\", most_sim_pair)\n",
        "print(\"Least similar pair:\", least_sim_pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmN8QAwZhzn7",
        "outputId": "23982b0d-4559-4045-d27f-e71f5222eb9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document index: 10\n",
            "Top tokens: ['blood', 'used', 'lamb', 'hard', 'task', 'culture', 'animal', 'sacrifice', 'something', 'related']\n",
            "\n",
            "Document index: 25\n",
            "Top tokens: ['anyone', 'brief', 'blurb', 'manned', 'lunar', 'exploration', 'confernce', 'may', 'crystal', 'city']\n",
            "\n",
            "Document index: 200\n",
            "Top tokens: ['jesus', 'peace']\n",
            "Jaccard similarity between doc 10 and doc 25: 0.000\n",
            "Jaccard similarity between doc 10 and doc 200: 0.000\n",
            "Jaccard similarity between doc 25 and doc 200: 0.000\n",
            "\n",
            "Most similar pair: ((10, 25), 0.0)\n",
            "Least similar pair: ((10, 25), 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: create simple tables (pandas) for topics and doc-topic scores (optional)\n",
        "import pandas as pd\n",
        "\n",
        "# NMF topic table\n",
        "nmf_topic_table = pd.DataFrame({\n",
        "    f\"Topic_{i+1}\": nmf_topics[i] for i in range(n_topics)\n",
        "})\n",
        "nmf_topic_table.index = [f\"Top_{i+1}\" for i in range(n_top_words)]\n",
        "print(nmf_topic_table)\n",
        "\n",
        "# Document-topic matrix quick look (for first 10 docs)\n",
        "doc_topic_df = pd.DataFrame(W[:10], columns=[f\"Topic_{i+1}\" for i in range(n_topics)])\n",
        "print(doc_topic_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6zZkmm-h3ki",
        "outputId": "e8241de9-facc-4bee-bfeb-7c61c7814e3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Topic_1  Topic_2   Topic_3     Topic_4    Topic_5\n",
            "Top_1    would   window      game       drive        god\n",
            "Top_2      one     file      team        scsi  christian\n",
            "Top_3   people   thanks      year        disk      jesus\n",
            "Top_4     like   please    player        card      bible\n",
            "Top_5    think  program    hockey        hard    believe\n",
            "Top_6      get   anyone    season  controller     christ\n",
            "Top_7    right     know      play         ide        say\n",
            "Top_8     time     mail  baseball      floppy     belief\n",
            "Top_9     know       do       win         mac      faith\n",
            "Top_10   thing   driver      last         meg        sin\n",
            "    Topic_1   Topic_2   Topic_3   Topic_4   Topic_5\n",
            "0  0.007299  0.000000  0.094179  0.001667  0.000457\n",
            "1  0.000000  0.033600  0.000159  0.048721  0.000000\n",
            "2  0.042391  0.000000  0.000000  0.000000  0.000000\n",
            "3  0.000000  0.000000  0.000000  0.089181  0.000000\n",
            "4  0.011746  0.026967  0.000000  0.101664  0.001811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-O1aoawiC0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}